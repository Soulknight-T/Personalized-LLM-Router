from __future__ import annotations

import os
import json
import random
from collections import defaultdict
from dataclasses import dataclass
from enum import Enum
from typing import (
    Set, List, Dict, Tuple, Union, Optional, Callable, Iterator, TypeAlias
)

import torch
from transformers import AutoTokenizer, AutoModel
from torch_geometric.data import HeteroData
from tqdm import tqdm 

from torch_geometric.utils import to_torch_coo_tensor as to_sparse_tensor

# --- Types & constants ---

InteractionUnit = Tuple[int, int, int, int, int]  # (user, session, query, llm, response)
TensorOrEmbInit = Union["EmbInit", torch.Tensor]
EncodeFn = Callable[..., torch.Tensor]

NodeDict: TypeAlias = Dict[str, torch.Tensor]
EdgeIndexDict: TypeAlias = Dict["HeteroEdges", torch.Tensor]
EdgeBag: TypeAlias = Dict["HeteroEdges", List[Tuple[int, int, Optional[float]]]]

DTYPE = torch.float32


def _default_device() -> torch.device:
    return torch.device("cuda" if torch.cuda.is_available() else "cpu")


# --- Enums ---

class EmbInit(str, Enum):
    ZEROS = "zero"
    ONES = "ones"
    RANDOM = "random"


class HeteroEdges(Tuple[str, str, str], Enum):
    U2S = ("user", "own", "session")
    U2Q = ("user", "ask", "query")
    U2R = ("user", "receive", "response")
    S2U = ("session", "owned by", "user")
    S2Q = ("session", "include", "query")
    S2R = ("session", "include", "response")
    S2L = ("session", "call", "llm")
    S1S2 = ("session", "next", "session")
    S2S1 = ("session", "prev", "session")
    Q2U = ("query", "asked by", "user")
    Q2S = ("query", "included by", "session")
    Q2R = ("query", "answered by", "response")
    Q2L = ("query", "to", "llm")
    Q1Q2 = ("query", "next", "query")
    Q2Q1 = ("query", "prev", "query")
    R2U = ("response", "back to", "user")
    R2S = ("response", "included by", "session")
    R2Q = ("response", "answered to", "query")
    R2L = ("response", "generated by", "llm")
    L2Q = ("llm", "receive", "query")
    L2R = ("llm", "generate", "response")
    L2S = ("llm", "response to", "session")


# --- Feature container ---

@dataclass
class HeteroFeature:
    """Holds dataset sizes and required feature tensors."""
    num_users: int
    num_sessions: int
    num_queries: int
    num_responses: int
    num_llms: int

    query_emb: torch.Tensor      # [num_queries, emb_dim]
    llm_plm_emb: torch.Tensor    # [num_llms,   emb_dim]

    aggregation_type: str = "mean"
    emb_dim: int = 768
    user_emb: TensorOrEmbInit = EmbInit.ZEROS
    session_emb: TensorOrEmbInit = EmbInit.ZEROS
    response_emb: TensorOrEmbInit = EmbInit.ZEROS
    eps: float = 1e-8

    plm_name: Optional[str] = None
    plm_encode: Optional[EncodeFn] = None

    def __post_init__(self):
        for k in ("num_users", "num_sessions", "num_queries", "num_responses", "num_llms"):
            v = getattr(self, k)
            if not isinstance(v, int):
                raise ValueError(f"{k} must be a positive int, got {v}")
        
        if not (isinstance(self.query_emb, torch.Tensor) and self.query_emb.shape[0] == self.num_queries):
            raise ValueError(f"query_emb shape 0 mismatch {self.query_emb.shape[0]} vs {self.num_queries}")
        if not (isinstance(self.llm_plm_emb, torch.Tensor) and self.llm_plm_emb.shape[0] == self.num_llms):
            raise ValueError(f"llm_plm_emb shape 0 mismatch {self.llm_plm_emb.shape[0]} vs {self.num_llms}")
        
        if self.query_emb.shape[0] > 0:
            self.emb_dim = self.query_emb.shape[1]
        elif self.llm_plm_emb.shape[0] > 0:
            self.emb_dim = self.llm_plm_emb.shape[1]
            
    def _tinfo(self, t: Optional[torch.Tensor]) -> str:
        return "None" if t is None else f"shape={tuple(t.shape)}, dtype={t.dtype}, device={t.device}"

    def _init_info(self, val: TensorOrEmbInit, rows: int) -> str:
        if isinstance(val, torch.Tensor):
            ok = (val.dim() == 2 and val.size(0) == rows and val.size(1) == self.emb_dim)
            shape = tuple(val.shape)
            return f"Tensor[{shape}, dtype={val.dtype}, device={val.device}]{'' if ok else ' (mismatch!)'}"
        return f"init={val.value}"

    def summary(self) -> str:
        enc_name = getattr(self.plm_encode, "__name__", self.plm_encode.__class__.__name__) if self.plm_encode else None
        return "\n".join([
            "HeteroFeatureConfig(",
            f"  counts: users={self.num_users}, sessions={self.num_sessions}, queries={self.num_queries}, responses={self.num_responses}, llms={self.num_llms}",
            f"  emb_dim={self.emb_dim}",
            f"  user_emb:    {self._init_info(self.user_emb, self.num_users)}",
            f"  session_emb: {self._init_info(self.session_emb, self.num_sessions)}",
            f"  query_emb:   {self._tinfo(self.query_emb)}",
            f"  response_emb:{self._init_info(self.response_emb, self.num_responses)}",
            f"  llm_plm_emb: {self._tinfo(self.llm_plm_emb)}",
            f"  plm: name='{self.plm_name}', encode={enc_name}",
            ")"
        ])

    __repr__ = __str__ = summary


# --- PLM encoding ---

def make_plm_encode(plm_name: str) -> Callable[[List[str]], torch.Tensor]:
    """Return mean-pooled HF encoder(texts) -> [B, H] float32."""
    device = _default_device()
    tokenizer = AutoTokenizer.from_pretrained(plm_name)
    model = AutoModel.from_pretrained(plm_name).to(device).eval()

    def encode_fn(texts: List[str]) -> torch.Tensor:
        with torch.no_grad():
            toks = tokenizer(texts, max_length=512, padding=True, truncation=True, return_tensors="pt")
            toks = {k: v.to(device) for k, v in toks.items()}
            out = model(**toks)
            hidden = out.last_hidden_state                   # [B,T,H]
            attn = toks["attention_mask"].bool()             # [B,T]
            hidden = hidden.masked_fill(~attn[..., None], 0) # mask padding
            denom = attn.sum(dim=1, keepdim=True).clamp_min(1)
            return (hidden.sum(dim=1) / denom).to(dtype=DTYPE)
    return encode_fn


# --- Edge helpers ---

def add_edge(
    edges_bag: EdgeBag,
    edge_type: HeteroEdges,
    src: Optional[int],
    dst: Optional[int],
    weight: Optional[float] = None
) -> None:
    if src is None or dst is None:
        return
    edges_bag[edge_type].append((src, dst, weight))


def build_interaction_edges(
    edges_bag: EdgeBag,
    interaction1: InteractionUnit,
    interaction2: InteractionUnit,
    prev_s1_idx: Optional[int],
    prev_s2_idx: Optional[int],
) -> None:
    u1, s1, q1, m1, r1 = interaction1
    u2, s2, q2, m2, r2 = interaction2

    for (etype, src, dst) in (
        (HeteroEdges.S2U, s1, u1),
        (HeteroEdges.S2U, s2, u2),
        (HeteroEdges.Q2S, q1, s1),
        (HeteroEdges.Q2S, q2, s2),
        (HeteroEdges.L2S, m1, s1),
        (HeteroEdges.L2S, m2, s2),
        (HeteroEdges.R2S, r1, s1),
        (HeteroEdges.R2S, r2, s2),
        (HeteroEdges.S2S1, s1, prev_s1_idx),
        (HeteroEdges.S2S1, s2, prev_s2_idx),
    ):
        add_edge(edges_bag, etype, src, dst)


def build_graph_edges(
    edges_bag: EdgeBag,
    device: torch.device
) -> EdgeIndexDict:
    edge_index_dict: EdgeIndexDict = {}
    for etype, triples in edges_bag.items():
        if not triples:
            continue
        src, dst, _ = zip(*triples)
        edge_index_dict[etype] = torch.stack(
            (
                torch.tensor(src, dtype=torch.long, device=device),
                torch.tensor(dst, dtype=torch.long, device=device),
            ),
            dim=0,
        )
    return edge_index_dict


# --- Edge views / session helpers ---

def _as_edges_dict(graph: Union[HeteroData, EdgeIndexDict]) -> EdgeIndexDict:
    if isinstance(graph, HeteroData):
        out: EdgeIndexDict = {}
        for et in graph.edge_types:
            ei = graph[et].edge_index
            if ei is None: continue
            try: et_enum = HeteroEdges(et)
            except ValueError: continue
            out[et_enum] = ei
        return out
    return graph

def build_prev_session_map(graph: Union[HeteroData, EdgeIndexDict]) -> Dict[int, int]:
    edges = _as_edges_dict(graph)
    s2s1 = edges.get(HeteroEdges.S2S1)
    if s2s1 is None: return {}
    curr, prev = s2s1[0].tolist(), s2s1[1].tolist()
    return dict(zip(curr, prev))


# --- JSONL parsing/building ---

def parse_jsonl_pairs(path: str) -> Iterator[Tuple[dict, dict]]:
    with open(path, "r", encoding="utf-8") as f:
        it = iter(f)
        for line1 in it:
            line2 = next(it, None)
            if line2 is None:
                break
            try:
                yield json.loads(line1), json.loads(line2)
            except json.JSONDecodeError:
                print(f"Skipping invalid JSONL line: {line1.strip()}")


def build_nodes_from_pairs(pairs_iterator: Iterator[Tuple[dict, dict]]):
    users: Dict[int, str] = {}
    llms: Dict[int, str] = {}
    queries: Dict[int, str] = {}
    responses: Dict[int, str] = {}

    user_to_idx: Dict[str, int] = {}
    llm_to_idx: Dict[str, int] = {}
    query_to_idx: Dict[str, int] = {}
    response_to_idx: Dict[str, int] = {}

    query_emb: List[List[float]] = []
    llm_emb: List[List[float]] = []
    
    edges_buffer: EdgeBag = defaultdict(list)
    preference_pairs: Set[Tuple[InteractionUnit, InteractionUnit]] = set()

    plm_name: Optional[str] = None
    session_cnt = 0
    
    emb_dim = 768 
    found_dim = False

    def get_embedding(data: dict, key: str, default_dim: int) -> Tuple[list, bool]:
        nonlocal found_dim, emb_dim
        emb = data.get(key)
        if emb and isinstance(emb, list):
            if not found_dim:
                emb_dim = len(emb)
                found_dim = True
            
            if len(emb) != emb_dim: 
                print(f"Embedding dimension mismatch. Expected {emb_dim}, got {len(emb)}. Padding with 0.")
                return [0.0] * emb_dim, False
            return emb, True
        else:
            return [0.0] * default_dim, False

    for e1, e2 in pairs_iterator:
        try:
            assert e1["question_id"] == e2["question_id"], "Question ID"
            assert e1["turn"] == e2["turn"], "Turn"
            u1 = e1["judge"]; u2 = e2["judge"]; assert u1 == u2, "Judge"
        except AssertionError as e:
            print(f"Skipping inconsistent JSONL pair: {e}")
            continue

        u_idx = user_to_idx.get(u1)
        if u_idx is None:
            u_idx = len(user_to_idx); user_to_idx[u1] = u_idx; users[u_idx] = u1

        l1 = e1["model"]; m1 = llm_to_idx.get(l1)
        if m1 is None:
            l1_emb_data, _ = get_embedding(e1, "model_emb", emb_dim)
            m1 = len(llm_to_idx); llm_to_idx[l1] = m1; llms[m1] = l1
            llm_emb.append(l1_emb_data)
            
        l2 = e2["model"]; m2 = llm_to_idx.get(l2)
        if m2 is None:
            l2_emb_data, _ = get_embedding(e2, "model_emb", emb_dim)
            m2 = len(llm_to_idx); llm_to_idx[l2] = m2; llms[m2] = l2
            llm_emb.append(l2_emb_data)

        prev1: Optional[int] = None
        prev2: Optional[int] = None
        
        conv1 = e1.get("conversation", [])
        conv2 = e2.get("conversation", [])
        if not (isinstance(conv1, list) and isinstance(conv2, list)):
            continue
            
        for t1, t2 in zip(conv1, conv2):
            s1 = session_cnt; s2 = session_cnt + 1; session_cnt += 2
            
            q1_text = t1.get("query")
            if q1_text is None: continue
            q1_idx = query_to_idx.get(q1_text)
            if q1_idx is None:
                q1_emb_data, emb_found = get_embedding(t1, "query_emb", emb_dim)
                if not emb_found and not found_dim:
                     continue
                
                q1_idx = len(query_emb); query_to_idx[q1_text] = q1_idx
                queries[q1_idx] = q1_text; query_emb.append(q1_emb_data)

            q2_text = t2.get("query")
            if q2_text is None: continue
            q2_idx = query_to_idx.get(q2_text)
            if q2_idx is None:
                q2_emb_data, emb_found = get_embedding(t2, "query_emb", emb_dim)
                if not emb_found and not found_dim:
                     continue
                
                q2_idx = len(query_emb); query_to_idx[q2_text] = q2_idx
                queries[q2_idx] = q2_text; query_emb.append(q2_emb_data)
            
            r1_text = t1.get("response", "None")
            r1_idx = response_to_idx.get(r1_text)
            if r1_idx is None:
                r1_idx = len(responses); response_to_idx[r1_text] = r1_idx
                responses[r1_idx] = r1_text

            r2_text = t2.get("response", "None")
            r2_idx = response_to_idx.get(r2_text)
            if r2_idx is None:
                r2_idx = len(responses); response_to_idx[r2_text] = r2_idx
                responses[r2_idx] = r2_text

            i1 = (u_idx, s1, q1_idx, m1, r1_idx)
            i2 = (u_idx, s2, q2_idx, m2, r2_idx)
            
            rating1 = float(t1.get("rating", 0.0))
            rating2 = float(t2.get("rating", 0.0))

            if rating1 > rating2:
                preference_pairs.add((i1, i2)) 
            elif rating2 > rating1:
                preference_pairs.add((i2, i1)) 

            build_interaction_edges(edges_buffer, i1, i2, prev1, prev2)

            add_edge(edges_buffer, HeteroEdges.Q2U, q1_idx, u_idx)
            add_edge(edges_buffer, HeteroEdges.Q2U, q2_idx, u_idx)
            add_edge(edges_buffer, HeteroEdges.L2Q, m1, q1_idx)
            add_edge(edges_buffer, HeteroEdges.L2Q, m2, q2_idx)
            
            prev1, prev2 = s1, s2

        if plm_name is None and "encoder" in e1:
            plm_name = e1["encoder"]

    return (
        users, llms, queries, responses, 
        query_emb, llm_emb, 
        preference_pairs, 
        edges_buffer, plm_name, session_cnt,
        user_to_idx, llm_to_idx, query_to_idx, response_to_idx, 
        emb_dim 
    )


# --- Build from JSONL ---

def build_graph_from_jsonl(
    train_jsonl_path: str 
):

    device = _default_device()
    (
        users, llms, queries, responses, 
        q_emb, l_emb, 
        train_pairs, 
        edge_buf, plm, sess_n,
        user_to_idx, llm_to_idx, query_to_idx, response_to_idx,
        inferred_emb_dim 
    ) = build_nodes_from_pairs(parse_jsonl_pairs(train_jsonl_path))

    if sess_n == 0:
        raise ValueError("No sessions found in the provided JSONL data.")

    emb_dim = inferred_emb_dim

    full_edge_index_dict = build_graph_edges(edge_buf, device)
    
    x_user = torch.randn((len(users), emb_dim), dtype=DTYPE, device=device) * 0.02
    
    x_session = torch.zeros((sess_n, emb_dim), dtype=DTYPE, device=device)
    x_query = torch.tensor(q_emb, dtype=DTYPE, device=device) if q_emb else torch.empty((0, emb_dim), dtype=DTYPE, device=device)
    x_llm = torch.tensor(l_emb, dtype=DTYPE, device=device) if l_emb else torch.empty((0, emb_dim), dtype=DTYPE, device=device)
    x_response = torch.zeros((len(responses), emb_dim), dtype=DTYPE, device=device)

    nodes: NodeDict = { "user": x_user, "session": x_session, "response": x_response, "query": x_query, "llm": x_llm }

    config = HeteroFeature(
        num_users=len(users), num_sessions=sess_n, num_queries=len(q_emb),
        num_responses=len(responses), num_llms=len(l_emb),
        query_emb=x_query, llm_plm_emb=x_llm, 
        emb_dim=emb_dim, 
        plm_name=plm,
        plm_encode=(make_plm_encode(plm) if plm is not None else None),
    )

    metapath_edge_index_dict = {}
    try:
        num_users = config.num_users
        num_queries = config.num_queries
        num_llms = config.num_llms
        num_sessions = config.num_sessions
        
        # meta_LQU
        if HeteroEdges.L2Q in full_edge_index_dict and HeteroEdges.Q2U in full_edge_index_dict:
            adj_LQ = to_sparse_tensor(full_edge_index_dict[HeteroEdges.L2Q], size=(num_llms, num_queries))
            adj_QU = to_sparse_tensor(full_edge_index_dict[HeteroEdges.Q2U], size=(num_queries, num_users))
            adj_meta_LQU = torch.sparse.mm(adj_LQ, adj_QU); 
            row, col = adj_meta_LQU.indices()
            if row.numel() > 0:
                metapath_edge_index_dict[("llm", "meta_LQU", "user")] = torch.stack([row, col])
                print(f"meta_LQU: find {row.size(0)} paths")
            else:
                print("meta_LQU: no paths found.")
        else:
            print(f"meta_LQU: skipped (missing L2Q or Q2U edges)")

        # meta_LSU
        if HeteroEdges.L2S in full_edge_index_dict and HeteroEdges.S2U in full_edge_index_dict:
            adj_LS = to_sparse_tensor(full_edge_index_dict[HeteroEdges.L2S], size=(num_llms, num_sessions))
            adj_SU = to_sparse_tensor(full_edge_index_dict[HeteroEdges.S2U], size=(num_sessions, num_users))
            adj_meta_LSU = torch.sparse.mm(adj_LS, adj_SU); 
            row, col = adj_meta_LSU.indices()
            if row.numel() > 0:
                metapath_edge_index_dict[("llm", "meta_LSU", "user")] = torch.stack([row, col])
                print(f"meta_LSU: find {row.size(0)} paths")
            else:
                print("meta_LSU: no paths found.")
        else:
            print(f"meta_LSU: skipped (missing L2S or S2U edges)")

    except KeyError as e:
        print(f"KeyError: {e}")
    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()

    mapping_dicts = {
        "user_to_idx": user_to_idx, "llm_to_idx": llm_to_idx,
        "query_to_idx": query_to_idx, "response_to_idx": response_to_idx,
        "users": users, "llms": llms, "queries": queries, "responses": responses
    }

    return (
        config,
        nodes,
        metapath_edge_index_dict,
        mapping_dicts,
        device,
        list(train_pairs) 
    )


def load_pairs_from_jsonl(
    path: str,
    mapping_dicts: dict
) -> List[Tuple[InteractionUnit, InteractionUnit]]:
    
    if not os.path.exists(path):
        return []
    
    user_to_idx = mapping_dicts["user_to_idx"]
    llm_to_idx = mapping_dicts["llm_to_idx"]
    query_to_idx = mapping_dicts["query_to_idx"]
    response_to_idx = mapping_dicts["response_to_idx"]
    
    preference_pairs: List[Tuple[InteractionUnit, InteractionUnit]] = []
    
    session_map: Dict[str, int] = {} 
    session_cnt_offset = 1_000_000 
    
    query_not_found = 0
    user_not_found = 0
    llm_not_found = 0

    for e1, e2 in parse_jsonl_pairs(path):
        try:
            u_idx = user_to_idx.get(e1["judge"])
            if u_idx is None:
                user_not_found += 1
                raise StopIteration 
                
            m1 = llm_to_idx.get(e1["model"])
            m2 = llm_to_idx.get(e2["model"])
            if m1 is None or m2 is None:
                llm_not_found += 1
                raise StopIteration 
            
            conv1 = e1.get("conversation", [])
            conv2 = e2.get("conversation", [])
            if not (isinstance(conv1, list) and isinstance(conv2, list)):
                raise StopIteration

            for t1, t2 in zip(conv1, conv2):
                
                q1_idx = query_to_idx.get(t1.get("query"))
                q2_idx = query_to_idx.get(t2.get("query"))
                if q1_idx is None or q2_idx is None:
                    query_not_found += 1
                    raise StopIteration 
                
                r1_idx = response_to_idx.get(t1.get("response", "None"), -1) 
                r2_idx = response_to_idx.get(t2.get("response", "None"), -1) 
                
                s1_id_str = f"val_{e1['question_id']}_{e1['turn']}"
                s1_idx = session_map.get(s1_id_str)
                if s1_idx is None:
                    s1_idx = session_cnt_offset + len(session_map)
                    session_map[s1_id_str] = s1_idx
                
                s2_id_str = f"val_{e2['question_id']}_{e2['turn']}"
                s2_idx = session_map.get(s2_id_str)
                if s2_idx is None:
                    s2_idx = session_cnt_offset + len(session_map)
                    session_map[s2_id_str] = s2_idx

                i1 = (u_idx, s1_idx, q1_idx, m1, r1_idx)
                i2 = (u_idx, s2_idx, q2_idx, m2, r2_idx)

                rating1 = float(t1.get("rating", 0.0))
                rating2 = float(t2.get("rating", 0.0))
                
                if rating1 > rating2:
                    preference_pairs.append((i1, i2)) 
                elif rating2 > rating1:
                    preference_pairs.append((i2, i1)) 
        
        except StopIteration:
            continue 
        except KeyError as e:
            print(f"KeyError: {e}")

    return preference_pairs

def build_hetero_data(nodes: NodeDict, edge_index_dict: EdgeIndexDict) -> HeteroData:
    data = HeteroData()
    
    for node_type, node_tensor in nodes.items():
        if node_tensor.shape[0] > 0: 
            data[node_type].x = node_tensor
        
    for etype, edge_index in edge_index_dict.items():
        etype_tuple = etype.value if isinstance(etype, HeteroEdges) else etype
        src, rel, dst = etype_tuple

        if edge_index.shape[0] != 2:
            if edge_index.shape == (0,) or edge_index.numel() == 0: 
                edge_index = torch.empty((2, 0), dtype=torch.long, device=edge_index.device)
            else:
                raise ValueError(f"Edge index for {(src, rel, dst)} has invalid shape: {edge_index.shape}")
        
        if src in data.node_types and dst in data.node_types:
             data[(src, rel, dst)].edge_index = edge_index
        else:
            print(f"KeyError: skipped edge {(src, rel, dst)} (missing nodes {src} or {dst})")

    return data